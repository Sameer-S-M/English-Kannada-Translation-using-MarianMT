# -*- coding: utf-8 -*-
"""Micoworks_final_pakka.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10IyXqE__ygqBGD0YYKzkAdDkikZgTG4m
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import torch
import math
from torch import nn
import torch.nn.functional as F

english_file = '/content/drive/MyDrive/Micoworks/train_30k.en'
kannada_file = '/content/drive/MyDrive/Micoworks/train_30k.kn'

START_TOKEN = '<START>'
PADDING_TOKEN = '<PADDING>'
END_TOKEN = '<END>'

# Kannada Alphabet, Numerals, and Symbols
kannada_vocabulary = [START_TOKEN, ' ', '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', '-', '.', '/',
                       ':', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`',

    # Kannada Vowels (Swaras)
    'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ',

    # Kannada Consonants (Vyanjanas)
    'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ',
    'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ',
    'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ',
    'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ',
    'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ',
    'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ',

    # Kannada Diacritics (Matras)
    'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್',

    # Kannada Numerals
    '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯',

    # Kannada Special Characters
    'ಂ', 'ಃ', 'ೞ', 'ೣ', '಼', 'ಽ',

    # Additional Symbols Used in Kannada Texts
    '₹', '€', '¥', '¢', '£', '©', '®', '™', '°', '±', '÷', '×', '≤', '≥', '∞', '∑', '∏', '∂',
    '∫', '√', '≈', '≠', '←', '↑', '→', '↓', '⇄', '⇌', '∩', '∪', '⊂', '⊃', '⊆', '⊇', '∧', '∨', '∃', '∀', '¬', '∈', '∉',
    '∅', '∇', '∝', 'ℓ', 'ℵ',

    # Ending Tokens
    PADDING_TOKEN, END_TOKEN
]

english_vocabulary = [START_TOKEN, ' ', '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', '-', '.', '/',
                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
                        ':', '<', '=', '>', '?', '@',
                        '[', '\\', ']', '^', '_', '`',
                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',
                        'y', 'z',
                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]

index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}
kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}
index_to_english = {k:v for k,v in enumerate(english_vocabulary)}
english_to_index = {v:k for k,v in enumerate(english_vocabulary)}

with open(english_file, 'r') as file:
    english_sentences = file.readlines()
with open(kannada_file, 'r', encoding='utf-8') as file:
    kannada_sentences = file.readlines()

TOTAL_SENTENCES = 30000
english_sentences = english_sentences[:TOTAL_SENTENCES]
kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]
english_sentences = [sentence.rstrip('\n').lower() for sentence in english_sentences]
kannada_sentences = [sentence.rstrip('\n') for sentence in kannada_sentences]

english_sentences[:10]

kannada_sentences[:15]

max_sequence_length = 270

def is_valid_tokens(sentence, vocab):
    for token in list(set(sentence)):
        if token not in vocab:
            return False
    return True

def is_valid_length(sentence, max_sequence_length):
    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space

valid_sentence_indicies = []
for index in range(len(kannada_sentences)):
    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]
    if is_valid_length(kannada_sentence, max_sequence_length) \
      and is_valid_length(english_sentence, max_sequence_length) \
      and is_valid_tokens(kannada_sentence, kannada_vocabulary):
        valid_sentence_indicies.append(index)

print(f"Number of sentences: {len(kannada_sentences)}")
print(f"Number of valid sentences: {len(valid_sentence_indicies)}")

kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]
english_sentences = [english_sentences[i] for i in valid_sentence_indicies]

kannada_sentences[:5]

english_sentences[:5]

!pip install transformers datasets sacrebleu sentencepiece torch

"""#MarianMT

"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

from transformers import MarianMTModel, MarianTokenizer



model_name = "Helsinki-NLP/opus-mt-en-mul"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name).to(device)

from datasets import Dataset


# Define a function to tokenize the dataset
def encode_batch(batch):
    inputs = tokenizer(batch["english"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")
    targets = tokenizer(batch["kannada"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")

    return {
        "input_ids": inputs["input_ids"].squeeze().to(device),
        "attention_mask": inputs["attention_mask"].squeeze().to(device),
        "labels": targets["input_ids"].squeeze().to(device)
    }

# Convert lists into Hugging Face Dataset
raw_dataset = Dataset.from_dict({"english": english_sentences, "kannada": kannada_sentences})

# Tokenize dataset
tokenized_dataset = raw_dataset.map(encode_batch, batched=True)

# Split into train and validation
train_size = int(0.9 * len(tokenized_dataset))  # 90% training, 10% validation
train_dataset = tokenized_dataset.select(range(train_size))
val_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./marianmt-finetuned-kn",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=64,  # Adjust based on GPU memory
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=6,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=500,
    save_strategy="epoch",
    fp16=True if torch.cuda.is_available() else False,  # Mixed precision for faster training
    push_to_hub=False,
    report_to="none"  # Disable W&B logging

)

pip install 'accelerate>=0.26.0'

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,  # Already moved to CUDA
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

model.save_pretrained("./Aastha-marianmt-finetuned-kn")
tokenizer.save_pretrained("./Aastha-marianmt-finetuned-kn")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

from transformers import MarianMTModel, MarianTokenizer

# Load the fine-tuned model
model = MarianMTModel.from_pretrained("/content/drive/MyDrive/Micoworks/KLE-MarianMT-en-to-kn2").to(device)
tokenizer = MarianTokenizer.from_pretrained("/content/drive/MyDrive/Micoworks/KLE-MarianMT-en-to-kn2")

"The product quality exceeded my expectations. Very satisfied with the purchase."
"Excellent customer support. Resolved my issue within a day."
"Disappointed with the performance. Expected better for the price."
"Color was different from what was shown in the images."
"It works well but took longer than expected to arrive."

# Example English text
english_text = ["The product quality exceeded my expectations. Very satisfied with the purchase.",
"Excellent customer support. Resolved my issue within a day.",
"Disappointed with the performance. Expected better for the price.",
"Color was different from what was shown in the images.",
"It works well but took longer than expected to arrive.",]
inputs = tokenizer(english_text, return_tensors="pt", padding=True, truncation=True).to(device)

# Generate translation
outputs = model.generate(**inputs)
translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]

print(translated_text)  # Kannada translation

for en,kn in zip(english_text,translated_text):
  print(en + " : " + kn)
  print()