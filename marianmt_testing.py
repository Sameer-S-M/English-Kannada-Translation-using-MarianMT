# -*- coding: utf-8 -*-
"""MarianMT_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmRleCL88JxRm04UjUStDb3agxsVzzjA
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

import numpy as np
import torch
import math
from torch import nn
import torch.nn.functional as F

"""#MarianMT

"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

from transformers import MarianMTModel, MarianTokenizer

model = MarianMTModel.from_pretrained("/content/drive/MyDrive/Micoworks/KLE-MarianMT-en-to-kn2").to(device)
tokenizer = MarianTokenizer.from_pretrained("/content/drive/MyDrive/Micoworks/KLE-MarianMT-en-to-kn2")

english_text = ["Fantastic quality and performance! Highly recommend for anyone needing a reliable solution",
                    "Works as expected, but feels a bit basic. A good budget option, perhaps",
                    "Arrived broken and didn't perform as advertised. A complete waste of money, unfortunately"]
inputs = tokenizer(english_text, return_tensors="pt", padding=True, truncation=True).to(device)

outputs = model.generate(**inputs)
translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]

for en, kn in zip(english_text, translated_text):
    print(f"English: {en}")
    print(f"Translated: {kn}")
    print("\n" + "="*40 )

"""#Eval

"""

pip install nltk

import nltk
nltk.download('punkt')

!pip install sacrebleu

# Function to translate sentences using MarianMT
def translate_sentence(sentence, tokenizer, model, device):
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True).to(device)  # Move inputs to GPU
    translated = model.generate(**inputs)  # Generate translation
    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
    return translated_text

from transformers import MarianMTModel, MarianTokenizer
import sacrebleu
import torch
from nltk.translate.meteor_score import meteor_score

# Load the custom-trained MarianMT model and tokenizer
model_name = "/content/drive/MyDrive/Micoworks/KLE-MarianMT-en-to-kn2"  # Replace with your model's directory
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = MarianMTModel.from_pretrained(model_name).to(device)  # Move model to GPU
tokenizer = MarianTokenizer.from_pretrained(model_name)



# Load your test data
source_file = "/content/drive/MyDrive/Micoworks/test.en"  # Replace with your test data path
reference_file = "/content/drive/MyDrive/Micoworks/test.kn"  # Replace with your reference file

with open(source_file, 'r', encoding='utf-8') as f:
    source_sentences = f.readlines()

with open(reference_file, 'r', encoding='utf-8') as f:
    reference_sentences = f.readlines()

assert len(source_sentences) == len(reference_sentences), "Mismatch between source and reference sentences!"

predicted_sentences = [translate_sentence(sentence.strip(), tokenizer, model, device) for sentence in source_sentences]

predicted_sentences[:5]

bleu = sacrebleu.corpus_bleu(predicted_sentences, [reference_sentences])
print(f"BLEU score: {bleu.score}")

nltk.download('punkt_tab')
nltk.download('wordnet')

meteor_scores = [
    meteor_score(
        [nltk.word_tokenize(ref.strip())],  # Tokenize reference sentence
        nltk.word_tokenize(pred.strip())   # Tokenize predicted sentence
    )
    for ref, pred in zip(reference_sentences, predicted_sentences)
]

average_meteor = sum(meteor_scores) / len(meteor_scores)
print(f"METEOR score: {average_meteor}")